<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Spark编程指南 - Spark 1.5.1 Documentation</title>
        
          <meta name="description" content="Spark 1.5.1 programming guide in Java, Scala and Python">
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        

    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="http://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html">
                      <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">1.5.1</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">Overview</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Programming Guides<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">Quick Start</a></li>
                                <li><a href="programming-guide.html">Spark Programming Guide</a></li>
                                <li class="divider"></li>
                                <li><a href="streaming-programming-guide.html">Spark Streaming</a></li>
                                <li><a href="sql-programming-guide.html">DataFrames and SQL</a></li>
                                <li><a href="mllib-guide.html">MLlib (Machine Learning)</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX (Graph Processing)</a></li>
                                <li><a href="bagel-programming-guide.html">Bagel (Pregel on Spark)</a></li>
                                <li><a href="sparkr.html">SparkR (R on Spark)</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API Docs<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">Scala</a></li>
                                <li><a href="api/java/index.html">Java</a></li>
                                <li><a href="api/python/index.html">Python</a></li>
                                <li><a href="api/R/index.html">R</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Deploying<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">Overview</a></li>
                                <li><a href="submitting-applications.html">Submitting Applications</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark Standalone</a></li>
                                <li><a href="running-on-mesos.html">Mesos</a></li>
                                <li><a href="running-on-yarn.html">YARN</a></li>
                                <li class="divider"></li>
                                <li><a href="ec2-scripts.html">Amazon EC2</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">More<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">Configuration</a></li>
                                <li><a href="monitoring.html">Monitoring</a></li>
                                <li><a href="tuning.html">Tuning Guide</a></li>
                                <li><a href="job-scheduling.html">Job Scheduling</a></li>
                                <li><a href="security.html">Security</a></li>
                                <li><a href="hardware-provisioning.html">Hardware Provisioning</a></li>
                                <li><a href="hadoop-third-party-distributions.html">3<sup>rd</sup>-Party Hadoop Distros</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">Building Spark</a></li>
                                <li><a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">Contributing to Spark</a></li>
                                <li><a href="https://cwiki.apache.org/confluence/display/SPARK/Supplemental+Spark+Projects">Supplemental Projects</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v1.5.1</span></p>-->
                </div>
            </div>
        </div>

        <div class="container" id="content">
          
            <h1 class="title">Spark编程指南</h1>
          

          <ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">概览</a></li>
  <li><a href="#spark" id="markdown-toc-spark">连接Spark</a></li>
  <li><a href="#spark-1" id="markdown-toc-spark-1">初始化Spark</a>    <ul>
      <li><a href="#spark-shell" id="markdown-toc-spark-shell">使用Spark shell</a></li>
    </ul>
  </li>
  <li><a href="#rdds" id="markdown-toc-rdds">弹性分布式数据集(RDDs)</a>    <ul>
      <li><a href="#section-1" id="markdown-toc-section-1">并行化的集合</a></li>
      <li><a href="#section-2" id="markdown-toc-section-2">外部数据集</a></li>
      <li><a href="#rdd" id="markdown-toc-rdd">RDD操作</a>        <ul>
          <li><a href="#section-3" id="markdown-toc-section-3">基础</a></li>
          <li><a href="#spark-2" id="markdown-toc-spark-2">向Spark传递函数</a></li>
          <li><a href="#a-nameclosureslinka" id="markdown-toc-a-nameclosureslinka">理解闭包 <a name="ClosuresLink"></a></a>            <ul>
              <li><a href="#section-4" id="markdown-toc-section-4">示例</a></li>
              <li><a href="#vs-" id="markdown-toc-vs-">本地模式 vs. 集群模式</a></li>
              <li><a href="#rdd-1" id="markdown-toc-rdd-1">打印RDD中的元素</a></li>
            </ul>
          </li>
          <li><a href="#key-value" id="markdown-toc-key-value">使用key-value键值对</a></li>
          <li><a href="#transformation" id="markdown-toc-transformation">转换(Transformation)</a></li>
          <li><a href="#actions" id="markdown-toc-actions">Actions</a></li>
          <li><a href="#shuffle" id="markdown-toc-shuffle">Shuffle操作</a>            <ul>
              <li><a href="#section-5" id="markdown-toc-section-5">背景</a></li>
              <li><a href="#section-6" id="markdown-toc-section-6">性能影响</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#rdd-2" id="markdown-toc-rdd-2">RDD持久化</a>        <ul>
          <li><a href="#section-7" id="markdown-toc-section-7">存储级别的选择</a></li>
          <li><a href="#section-8" id="markdown-toc-section-8">数据移除</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#section-9" id="markdown-toc-section-9">共享变量</a>    <ul>
      <li><a href="#broadcast-variable" id="markdown-toc-broadcast-variable">广播变量(broadcast variable)</a></li>
      <li><a href="#accumulator-a-nameaccumlinka" id="markdown-toc-accumulator-a-nameaccumlinka">累加器(Accumulator) <a name="AccumLink"></a></a></li>
    </ul>
  </li>
  <li><a href="#section-10" id="markdown-toc-section-10">发布到集群中</a></li>
  <li><a href="#javascalasparkjob" id="markdown-toc-javascalasparkjob">从Java/Scala中启动Spark作业(job)</a></li>
  <li><a href="#section-11" id="markdown-toc-section-11">单元测试</a></li>
  <li><a href="#section-12" id="markdown-toc-section-12">从1.0以前的版本升级</a></li>
  <li><a href="#section-13" id="markdown-toc-section-13">更多资料</a></li>
</ul>

<h1 id="section">概览</h1>

<p>从高层面来讲，每一个Spark应用都由一个用于运行用户的<code>main</code>函数并在一个集群上执行<em>并行操作</em>的<em>驱动程序</em>构成。
Spark所提供的最主要的抽象是一个<em>弹性分布式数据集</em>(RDD)，它是一组分布在集群中多个节点上可以被并行操作的元素集。
RDD可以来自于一个Hadoop文件系统中的文件(或任何Hadoop支持的文件系统)，或是一个已存在于驱动程序中的Scala集合转换而来。
用户可以通过Spark将RDD<em>持久化</em>于内从中，从而可以使其在并行操作中高效的重用。而且RDD在节点失效后自动回复。</p>

<p>Spark所提供的第二个抽象是一组可被用于并行操作的<em>共享的变量</em>。默认情况下，当Spark在不同的节点上并行的将一个函数作为一组任务运行时，
它会将该函数中使用的每一个变量复制并携带进入各个任务中。有时一个变量需要被在多个任务中共享，或在任务与驱动程序之间共享。
Spark支持两类共享变量：用于在所有节点内存中缓存值的<em>广播变量</em>，以及<em>累加变量</em>&#8211;仅被用于进行&#8221;累加&#8221;操作，例如计数器和求和。</p>

<p>这篇指南将在Spark所支持的语言中展示以上特性。跟进的最简单的方式就是启动并使用Spark的交互式shell&#8211;无论是scala形式的<code>bin/spark-shell</code>还是Python
形式的<code>bin/pyspark</code>。</p>

<h1 id="spark">连接Spark</h1>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark 1.5.1使用了Scala 2.10。如果打算用Scala来写应用，
你需要使用一个相兼容的Scala版本(比如2.10.X)。</p>

    <p>如果打算编写Spark应用，你还需要将Spark添加进Maven依赖中。Spark依赖可以在Maven中央库中找到：</p>

    <pre><code>groupId = org.apache.spark
artifactId = spark-core_2.10
version = 1.5.1
</code></pre>

    <p>另外，如果你打算访问HDFS集群，你还需要为你的HDFS版本添加<code>hadoop-client</code>依赖。
<a href="hadoop-third-party-distributions.html">third party distributions</a>罗列了一些通用的HDFS版本标签。</p>

    <pre><code>groupId = org.apache.hadoop
artifactId = hadoop-client
version = &lt;your-hdfs-version&gt;
</code></pre>

    <p>最后，你需要将一些Spark类导入你的程序中。加入以下代码：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span></code></pre></div>

    <p>(在Spark 1.3.0之前，你需要显示的导入<code>import org.apache.spark.SparkContext._</code>来启用一些基本的隐式类型转换)</p>

  </div>

<div data-lang="java">

    <p>Spark 1.5.1需要Java 7或更高版本。如果你使用Java 8，Spark还支持用
<a href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html">lambda表达式</a>
来更简洁的书写函数，否则的话你需要使用在
<a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>
包下面的类。</p>

    <p>如果打算用Java编写Spark应用，你需要将Spark添加进Maven依赖中。Spark依赖可以在Maven中央库中找到：</p>

    <pre><code>groupId = org.apache.spark
artifactId = spark-core_2.10
version = 1.5.1
</code></pre>

    <p>另外，如果你打算访问HDFS集群，你还需要为你的HDFS版本添加<code>hadoop-client</code>依赖。
<a href="hadoop-third-party-distributions.html">third party distributions</a>罗列了一些通用的HDFS版本标签。</p>

    <pre><code>groupId = org.apache.hadoop
artifactId = hadoop-client
version = &lt;your-hdfs-version&gt;
</code></pre>

    <p>最后，你需要将一些Spark类导入你的程序中。加入以下代码：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.spark.api.java.JavaSparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.api.java.JavaRDD</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span></code></pre></div>

  </div>

<div data-lang="python">

    <p>Spark 1.5.1需要Python 2.6+或Python 3.4+。Spark可以使用标准的CPython解释器，
因而像NumPy一类的C库可以被使用。同样的，Spark也可以使用PyPy 2.3+。</p>

    <p>如果想使用Python启动Spark应用，请使用<code>bin/spark-submit</code>脚本。该脚本会将Spark Java/Scala库导入，并允许你
将应用提交到一个集群中。你可以使用<code>bin/pyspark</code>来启动交互式Python shell。</p>

    <p>如果你打算访问HDFS数据，你需要使用一个与你的HDFS版本相关的PySpark版本库。
<a href="hadoop-third-party-distributions.html">third party distributions</a>罗列了一些通用的HDFS版本标签。
Spark首页也罗列了通用HDFS版本可以使用的<a href="http://spark.apache.org/downloads.html">预构建包</a></p>

    <p>最后，你需要将一些Spark类导入你的程序中。加入以下代码：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span></code></pre></div>

    <p>PySpark需要在driver和workers中使用相同小版本的Python。它使用了PATH变量中声明的默认的python，你也可以通过声明<code>PYSPARK_PYTHON</code>
来指明你所想使用的Python的版本，例如：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ PYSPARK_PYTHON</span><span class="o">=</span>python3.4 bin/pyspark
<span class="nv">$ PYSPARK_PYTHON</span><span class="o">=</span>/opt/pypy-2.5/bin/pypy bin/spark-submit examples/src/main/python/pi.py</code></pre></div>

  </div>

</div>

<h1 id="spark-1">初始化Spark</h1>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark程序首先要做的事情就是创建<a href="api/scala/index.html#org.apache.spark.SparkContext">SparkContext</a>对象, 它可以通知
Spark如何访问一个集群。为了创建<code>SparkContext</code>对象，你首先要建立一个包含你的应用程序信息的
<a href="api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a>对象。</p>

    <p>每个JVM中应当只有一个活跃的SparkContext。在创建一个新的之前，你需要<code>stop()</code>当前活跃的SparkContext。</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="n">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">)</span>
<span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span></code></pre></div>

  </div>

<div data-lang="java">

    <p>Spark程序首先要做的事情就是创建<a href="api/java/index.html?org/apache/spark/api/java/JavaSparkContext.html">JavaSparkContext</a>
对象，它可以通知Spark如何访问一个集群。为了创建<code>SparkContext</code>对象，你首先要建立一个包含你的应用程序信息的
<a href="api/java/index.html?org/apache/spark/SparkConf.html">SparkConf</a>对象。</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">SparkConf</span><span class="o">().</span><span class="na">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="na">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">);</span>
<span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span></code></pre></div>

  </div>

<div data-lang="python">

    <p>Spark程序首先要做的事情就是创建<a href="api/python/pyspark.html#pyspark.SparkContext">SparkContext</a>对象，
它可以通知Spark如何访问一个集群。为了创建<code>SparkContext</code>对象，你首先要建立一个包含你的应用程序信息的
<a href="api/python/pyspark.html#pyspark.SparkConf">SparkConf</a>对象。</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">appName</span><span class="p">)</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">master</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span></code></pre></div>

  </div>

</div>

<p>参数<code>appName</code>表示的是将来在集群UI中显示的你的程序的名字。参数<code>master</code>则是一个
<a href="submitting-applications.html#master-urls">Spark，Mesos或者YARN集群的URL</a>，
或是一个特殊的&#8221;local&#8221;字符串用于运行本地模式。在实际应用中，当运行于一个集群中时，
你不会想去硬编码一个<code>master</code>到你的程序中的，取而代之的则是
<a href="submitting-applications.html">使用<code>spark-submit</code>启动程序</a>接收该参数。
但是在本地测试或单元测试时，你可以传递&#8221;local&#8221;来在本地进程中运行Spark。</p>

<h2 id="spark-shell">使用Spark shell</h2>

<div class="codetabs">

<div data-lang="scala">

    <p>在Spark shell启动时，一个可以被解释器所感知的SparkContext会被同时创建，并存放于变量<code>sc</code>中。这时你自行创建的SparkContext将无法使用。
你可以在启动时通过<code>--master</code>参数来指定context所要连接的master实例，并且还可以通过<code>--jars</code>参数来指定你所打算加载入类路径的jar包(多个
的话使用逗号来分割)。你还可以通过<code>--packages</code>参数后加maven坐标(多个的话使用逗号分割)的方式来往shell会话中添加对应的依赖项(例如对应的
Spark包)。如果需要添加依赖项目所在的maven仓库(比如SonaType)，可以通过<code>--repositories</code>参数来添加。例如，如果打算在本地的四个CPU核
上运行<code>bin/spark-shell</code>，可以用如下方式：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/spark-shell --master <span class="nb">local</span><span class="o">[</span>4<span class="o">]</span></code></pre></div>

    <p>如果打算在类路径里添加<code>code.jar</code>，可以用如下方式：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/spark-shell --master <span class="nb">local</span><span class="o">[</span>4<span class="o">]</span> --jars code.jar</code></pre></div>

    <p>如果打算通过maven坐标的方式添加依赖：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/spark-shell --master <span class="nb">local</span><span class="o">[</span>4<span class="o">]</span> --packages <span class="s2">&quot;org.example:example:0.1&quot;</span></code></pre></div>

    <p>请使用<code>spark-shell --help</code>来查看完整的命令选项。事实上在幕后，<code>spark-shell</code>调用了更多通用的
<a href="submitting-applications.html"><code>spark-submit</code>脚本</a>.</p>

  </div>

<div data-lang="python">

    <p>在PySpark shell启动时，一个可以被解释器所感知的SparkContext会被同时创建，并存放于变量<code>sc</code>中。这时你自行创建的SparkContext将无法使用。
你可以在启动时通过<code>--master</code>参数来指定context所要连接的master实例，并且还可以通过<code>--py-files</code>参数来指定你所打算加载入运行时的python包
(.zip，.egg或者.py文件，多个的话使用逗号来分割)。你还可以通过<code>--packages</code>参数后加maven坐标(多个的话使用逗号分割)的方式来往shell会话中
添加对应的依赖项(例如对应的Spark包)。如果需要添加依赖项目所在的maven仓库(比如SonaType)，可以通过<code>--repositories</code>参数来添加。
当需要时，任何Spark包所要依赖的python依赖(在spark包的requirements.txt文件中有列出)需要手动的通过pip来进行安装。例如，如果打算在本地的
四个CPU核上运行<code>bin/spark-shell</code>，可以用如下方式：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/pyspark --master <span class="nb">local</span><span class="o">[</span>4<span class="o">]</span></code></pre></div>

    <p>如果打算在搜索路径里添加<code>code.py</code>(用以在后边<code>import code</code>)，可以用如下方式：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/pyspark --master <span class="nb">local</span><span class="o">[</span>4<span class="o">]</span> --py-files code.py</code></pre></div>

    <p>请使用<code>pyspark --help</code>来查看完整的命令选项。事实上在幕后，<code>pyspark</code>调用了更多通用的
<a href="submitting-applications.html"><code>spark-submit</code>脚本</a>.</p>

    <p>你也可以使用一个叫做<a href="http://ipython.org">IPython</a>的增强版Python解释器来启动PySpark Shell。PySpark可以
在IPython1.0.0及以上版本下工作。可以在运行<code>bin/pyspark</code>时通过设置<code>PYSPARK_DRIVER_PYTHON</code>环境变量来使用IPython。</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ PYSPARK_DRIVER_PYTHON</span><span class="o">=</span>ipython ./bin/pyspark</code></pre></div>

    <p>你可以通过设置<code>PYSPARK_DRIVER_PYTHON_OPTS</code>来定制化<code>ipython</code>命令。例如，启动一个带有PyLab plot支持的
<a href="http://ipython.org/notebook.html">IPython Notebook</a>：</p>

    <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ PYSPARK_DRIVER_PYTHON</span><span class="o">=</span>ipython <span class="nv">PYSPARK_DRIVER_PYTHON_OPTS</span><span class="o">=</span><span class="s2">&quot;notebook&quot;</span> ./bin/pyspark</code></pre></div>

    <p>在IPython Notebook服务器启动后，你可以在&#8221;Files&#8221;标签下创建一个新的的&#8221;Python 2&#8221; notebook。在notebook中，
你可以在试用Spark之前通过输入<code>%pylab inline</code>来使用其作为notebook的一部分。</p>

  </div>

</div>

<h1 id="rdds">弹性分布式数据集(RDDs)</h1>

<p>Spark整体上是围绕着一个叫做 <em>弹性分布式数据集</em> (RDD)的概念来展开的。它是一个可以并行操作并可以容错的元素集合。
我们可以使用两种方式来创建RDD：<em>并行化(parallelizing)</em> 一个驱动程序中的已存集合，或者引用一个外部存储系统中的数据集合，
比如一个共享的文件系统，HDFS，HBase，或者任何可以提供Hadoop InputFormat的数据源。</p>

<h2 id="section-1">并行化的集合</h2>

<div class="codetabs">

<div data-lang="scala">

    <p>并行化集合可以通过在你的驱动程序内的一个已存集合上(例如一个Scala的<code>Seq</code>)调用<code>SparkContext</code>的<code>parallelize</code>方法来获得。
集合内的元素会被拷贝并构成一个能够并行化操作的分布式数据集。例如以下代码展示了如何创建一个包含1到5的并行化集合：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">)</span>
<span class="k">val</span> <span class="n">distData</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">)</span></code></pre></div>

    <p>一旦被创建，这个分布式数据集(<code>distData</code>)就可以并行化操作了。例如，我们可以调用<code>distData.reduce((a,b) =&gt; a + b)</code>来将数组元素
进行累加。我们将在后面继续介绍分布式数据集的操作。</p>

  </div>

<div data-lang="java">

    <p>并行化集合可以通过在你的驱动程序内的一个已存集合上调用<code>JavaSparkContext</code>的<code>parallelize</code>方法来获得。
集合内的元素会被拷贝并构成一个能够并行化操作的分布式数据集。例如以下代码展示了如何创建一个包含1到5的并行化集合：</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">List</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">);</span></code></pre></div>

    <p>一旦被创建，这个分布式数据集(<code>distData</code>)就可以并行化操作了。例如，我们可以调用<code>distData.reduce((a, b) -&gt; a + b)</code>来将列表元素
进行累加。我们将在后面继续介绍分布式数据集的操作。</p>

    <p><strong>注意：</strong> *在本指南中，我们会经常使用Java 8 lambda语法来表示Java函数，如果你使用旧版本的Java，你可以通过实现
<a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>
包内的接口来达到同样的目的。我们将在后面更详细的介绍<a href="#passing-functions-to-spark">如何给Spark传递函数</a></p>

  </div>

<div data-lang="python">

    <p>并行化集合可以通过在你的驱动程序内的一个已存集合或迭代器上调用<code>SparkContext</code>的<code>parallelize</code>方法来获得。
集合内的元素会被拷贝并构成一个能够并行化操作的分布式数据集。例如以下代码展示了如何创建一个包含1到5的并行化集合：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code></pre></div>

    <p>一旦被创建，这个分布式数据集(<code>distData</code>)就可以并行化操作了。例如，我们可以调用<code>distData.reduce(lambda a, b: a + b)</code>来将列表元素
进行累加。我们将在后面继续介绍分布式数据集的操作。</p>

  </div>

</div>

<p>并行化集合的一个重要参数就是 <em>partitions</em> &#8211;用以分割数据集。Spark将在集群中的每一个分区上运行一个任务。
一般情况下你可以在你的及群众为每一个CPU分配2-4个分区。一般来说，Spark将会基于你的集群来自动的设置分区数量。
但你也可以通过<code>parallelize</code>方法的第二个参数来手动的设置(例如，<code>sc.parallelize(data,10)</code>)。
注意：代码中有些地方会使用slices(partitions的一个同义词)来保证向后兼容性。</p>

<h2 id="section-2">外部数据集</h2>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark可以从任何支持Hadoop的外部存储源中创建分布式数据集，这些源包括你本地的文件系统，HDFS，Cassandra，HBase，
<a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon S3</a>等等。Spark同样也支持文本文件，
<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>
以及任何其他的Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>。</p>

    <p>可以使用<code>SparkContext</code>的<code>textFile</code>方法来创建文本文件RDD。这个方法接受一个文件的URI(本地文件路径或<code>hdfs://</code>，<code>s3n://</code>，或其他合法URI)
并且将其构造成有文件文本行组成的集合。以下是一个样例调用：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">distFile</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="n">distFile</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">MappedRDD</span><span class="k">@</span><span class="mi">1</span><span class="n">d4cee08</span></code></pre></div>

    <p>一旦被创建，<code>distFile</code>就可以被用作数据集进行操作了。例如，我们可以通过<code>map</code>和<code>reduce</code>操作将所有行的长度进行累加：
<code>distFile.map(s =&gt; s.length).reduce((a, b) =&gt; a + b)</code>。</p>

    <p>使用Spark读取文件需要注意：</p>

    <ul>
      <li>
        <p>如果使用一个本地文件系统的路径，文件需要在所有的worker结点上都能通过相同路径访问到。无论是通过拷贝文件到所有节点还是使用网络挂载的共享文件系统。</p>
      </li>
      <li>
        <p>所有基于文件的Spark输入方法，包括<code>textFile</code>，都支持运行在目录，压缩文件以及通配符上。例如，你可以使用<code>textFile("/my/directory")</code>，<code>textFile("/my/directory/*.txt")</code>以及<code>textFile("/my/directory/*.gz")</code>来访问文件。</p>
      </li>
      <li>
        <p><code>textFile</code>方法也可以接受一个可选的参数来控制文件的分区。默认情况下，Spark会为每个文件块分配一个分区(HDFS下文件块默认大小为64M)，但你可以通过传递一个比较大的值来请求分配更多的分区。需要注意的是你不能请求比文件块数目更少的分区。</p>
      </li>
    </ul>

    <p>除了文本文件外，Spark Scala API也支持其他几种数据格式：</p>

    <ul>
      <li>
        <p><code>SparkContext.wholeTextFiles</code>允许你读取一个包含很多小文本文件的目录，并且以(文件名，文件内容)元组的方式返回每个文件。与<code>textFile</code>相反，<code>textFile</code>会将每个文件中的每一行作为一个记录来返回。</p>
      </li>
      <li>
        <p>对于<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>，可以使用SparkContext的<code>sequenceFile[K, V]</code>方法，其中<code>K</code>是以文件类型做为的键值，<code>V</code>是以文件本身作为的值。它们应当是Hadoop的<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html">Writable</a>接口的子类，就像<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html">IntWritable</a>和<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html">Text</a>。另外，Spark允许你指定一些通用Writable原生类型；例如，<code>sequenceFile[Int, String]</code>将会自动读取IntWritable和Text。</p>
      </li>
      <li>
        <p>对于其他Hadoop InputFormat，你可以使用<code>SparkContext.hadoopRDD</code>方法，它必须接受一个<code>JobConf</code>类型的参数，以及input format类，key类和value类。使用与设置一个具有输入源的Hadoop作业相同的方式来设置这些值。你也可以对基于&#8221;新&#8221;的MapReduce API(<code>org.apache.hadoop.mapreduce</code>)的InputFormat来使用<code>SparkContext.newAPIHadoopRDD</code>方法。</p>
      </li>
      <li>
        <p><code>RDD.saveAsObjectFile</code>以及<code>SparkContext.objectFile</code>支持将由一个序列化的Java对象构成的简单格式存储为一个RDD。虽然这种方式不会像专属的格式(例如Avro)那样高效，但它提供了一种简单的方式来保存任意的RDD。</p>
      </li>
    </ul>

  </div>

<div data-lang="java">

    <p>Spark可以从任何支持Hadoop的外部存储源中创建分布式数据集，这些源包括你本地的文件系统，HDFS，Cassandra，HBase，
<a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon S3</a>等等。Spark同样也支持文本文件，
<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>
以及任何其他的Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>。</p>

    <p>可以使用<code>SparkContext</code>的<code>textFile</code>方法来创建文本文件RDD。这个方法接受一个文件的URI(本地文件路径或<code>hdfs://</code>，<code>s3n://</code>，或其他合法URI)
并且将其构造成有文件文本行组成的集合。以下是一个样例调用：</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span></code></pre></div>

    <p>一旦被创建，<code>distFile</code>就可以被用作数据集进行操作了。例如，我们可以通过<code>map</code>和<code>reduce</code>操作将所有行的长度进行累加：
<code>distFile.map(s -&gt; s.length()).reduce((a, b) -&gt; a + b)</code>。</p>

    <p>使用Spark读取文件需要注意：</p>

    <ul>
      <li>
        <p>如果使用一个本地文件系统的路径，文件需要在所有的worker结点上都能通过相同路径访问到。无论是通过拷贝文件到所有节点还是使用网络挂载的共享文件系统。</p>
      </li>
      <li>
        <p>所有基于文件的Spark输入方法，包括<code>textFile</code>，都支持运行在目录，压缩文件以及通配符上。例如，你可以使用<code>textFile("/my/directory")</code>，<code>textFile("/my/directory/*.txt")</code>以及<code>textFile("/my/directory/*.gz")</code>来访问文件。</p>
      </li>
      <li>
        <p><code>textFile</code>方法也可以接受一个可选的参数来控制文件的分区。默认情况下，Spark会为每个文件块分配一个分区(HDFS下文件块默认大小为64M)，但你可以通过传递一个比较大的值来请求分配更多的分区。需要注意的是你不能请求比文件块数目更少的分区。</p>
      </li>
    </ul>

    <p>除了文本文件外，Spark Scala API也支持其他几种数据格式：</p>

    <ul>
      <li>
        <p><code>JavaSparkContext.wholeTextFiles</code>允许你读取一个包含很多小文本文件的目录，并且以(文件名，文件内容)元组的方式返回每个文件。与<code>textFile</code>相反，<code>textFile</code>会将每个文件中的每一行作为一个记录来返回。</p>
      </li>
      <li>
        <p>对于<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>，可以使用SparkContext的<code>sequenceFile[K, V]</code>方法，其中<code>K</code>是以文件类型做为的键值，<code>V</code>是以文件本身作为的值。它们应当是Hadoop的<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html">Writable</a>接口的子类，就像<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html">IntWritable</a>和<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html">Text</a>。另外，Spark允许你指定一些通用Writable原生类型；例如，<code>sequenceFile[Int, String]</code>将会自动读取IntWritable和Text。</p>
      </li>
      <li>
        <p>对于其他Hadoop InputFormat，你可以使用<code>JavaSparkContext.hadoopRDD</code>方法，它必须接受一个<code>JobConf</code>类型的参数，以及input format类，key类和value类。使用与设置一个具有输入源的Hadoop作业相同的方式来设置这些值。你也可以对基于&#8221;新&#8221;的MapReduce API(<code>org.apache.hadoop.mapreduce</code>)的InputFormat来使用<code>JavaSparkContext.newAPIHadoopRDD</code>方法。</p>
      </li>
      <li>
        <p><code>JavaRDD.saveAsObjectFile</code>以及<code>JavaSparkContext.objectFile</code>支持将由一个序列化的Java对象构成的简单格式存储为一个RDD。虽然这种方式不会像专属的格式(例如Avro)那样高效，但它提供了一种简单的方式来保存任意的RDD。</p>
      </li>
    </ul>

  </div>

<div data-lang="python">

    <p>PySpark可以从任何支持Hadoop的外部存储源中创建分布式数据集，这些源包括你本地的文件系统，HDFS，Cassandra，HBase，
<a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon S3</a>等等。Spark同样也支持文本文件，
<a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>
以及任何其他的Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>。</p>

    <p>可以使用<code>SparkContext</code>的<code>textFile</code>方法来创建文本文件RDD。这个方法接受一个文件的URI(本地文件路径或<code>hdfs://</code>，<code>s3n://</code>，或其他合法URI)
并且将其构造成有文件文本行组成的集合。以下是一个样例调用：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;data.txt&quot;</span><span class="p">)</span></code></pre></div>

    <p>一旦被创建，<code>distFile</code>就可以被用作数据集进行操作了。例如，我们可以通过<code>map</code>和<code>reduce</code>操作将所有行的长度进行累加：
<code>distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)</code>。</p>

    <p>使用Spark读取文件需要注意：</p>

    <ul>
      <li>
        <p>如果使用一个本地文件系统的路径，文件需要在所有的worker结点上都能通过相同路径访问到。无论是通过拷贝文件到所有节点还是使用网络挂载的共享文件系统。</p>
      </li>
      <li>
        <p>所有基于文件的Spark输入方法，包括<code>textFile</code>，都支持运行在目录，压缩文件以及通配符上。例如，你可以使用<code>textFile("/my/directory")</code>，<code>textFile("/my/directory/*.txt")</code>以及<code>textFile("/my/directory/*.gz")</code>来访问文件。</p>
      </li>
      <li>
        <p><code>textFile</code>方法也可以接受一个可选的参数来控制文件的分区。默认情况下，Spark会为每个文件块分配一个分区(HDFS下文件块默认大小为64M)，但你可以通过传递一个比较大的值来请求分配更多的分区。需要注意的是你不能请求比文件块数目更少的分区。</p>
      </li>
    </ul>

    <p>除了文本文件外，Spark Scala API也支持其他几种数据格式：</p>

    <ul>
      <li>
        <p><code>SparkContext.wholeTextFiles</code>允许你读取一个包含很多小文本文件的目录，并且以(文件名，文件内容)元组的方式返回每个文件。与<code>textFile</code>相反，<code>textFile</code>会将每个文件中的每一行作为一个记录来返回。</p>
      </li>
      <li>
        <p><code>RDD.saveAsObjectFile</code>以及<code>SparkContext.objectFile</code>支持将由pickled化的python对象构成的简单格式存储为一个RDD。批处理用于pickle序列化，默认的批处理数量为10。</p>
      </li>
      <li>
        <p>SequenceFile 以及 Hadoop Input/Output Format</p>
      </li>
    </ul>

    <p><strong>注意</strong> 该功能目前被标记为<code>实验性</code>，并且仅仅面向高级用户。它很有可能会在将来被基于Spark SQL的读/写支持所替换，并且在这种情况下Spark SQL会是优先推荐使用的方式。</p>

    <p><strong>Writable支持</strong></p>

    <p>PySpark SequenceFile支持从一个Java的键值对中装载一个RDD，支持将Writable转换为基本的Java类型，同样也支持使用
<a href="https://github.com/irmen/Pyrolite/">Pyrolite</a>来将生成的Java对象进行pickle化。当将一个键值对构成的RDD保存为一个SequenceFile时，PySpark
可以做相反的操作。它可以将Python对象反pickle化为Java对象，然后将之转化为Writable。以下的Writable可以被自动转化：</p>

    <table class="table">
<tr><th>Writable Type</th><th>Python Type</th></tr>
<tr><td>Text</td><td>unicode str</td></tr>
<tr><td>IntWritable</td><td>int</td></tr>
<tr><td>FloatWritable</td><td>float</td></tr>
<tr><td>DoubleWritable</td><td>float</td></tr>
<tr><td>BooleanWritable</td><td>bool</td></tr>
<tr><td>BytesWritable</td><td>bytearray</td></tr>
<tr><td>NullWritable</td><td>None</td></tr>
<tr><td>MapWritable</td><td>dict</td></tr>
</table>

    <p>数组无法开箱即用。当进行读写时用户需要指定自定义的<code>ArrayWritable</code>子类型。当写入时，用户还需要指定自定义的转换器来将数组转换为自定义的<code>ArrayWritable</code>字类型。
当读取时，默认的转换器会将自定义的<code>ArrayWritable</code>字类型转换为Java的<code>Object[]</code>，然后被pickle化为Python的tuple。
为了得到Python的原生数据类型的数组的<code>array.array</code>，用户需要自定义转换器。</p>

    <p><strong>保存和读取SequenceFiles</strong></p>

    <p>与文本文件相类似，SequenceFiles可以通过指定路径来被保存和读取。key和value类可以被指定，但对标准的Writable来说这些并不需要。</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s">&quot;a&quot;</span> <span class="o">*</span> <span class="n">x</span> <span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">saveAsSequenceFile</span><span class="p">(</span><span class="s">&quot;path/to/file&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">sequenceFile</span><span class="p">(</span><span class="s">&quot;path/to/file&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s">u&#39;a&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s">u&#39;aa&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">u&#39;aaa&#39;</span><span class="p">)]</span></code></pre></div>

    <p><strong>保存和读取其他Hadoop Input/Output Format</strong></p>

    <p>PySpark同样可以读取任意的Hadoop InputFormat以及写入任意的Hadoop OutputFormat，不管你使用的是<code>新</code>的还是<code>旧</code>的Hadoop MapReduce API。
如果需要的话，一个Hadoop配置可以以一个Python dict的方式被传入。这里就是一个使用Elasticsearch ESInputFormat的例子：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="err">$</span> <span class="n">SPARK_CLASSPATH</span><span class="o">=/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">elasticsearch</span><span class="o">-</span><span class="n">hadoop</span><span class="o">.</span><span class="n">jar</span> <span class="o">./</span><span class="nb">bin</span><span class="o">/</span><span class="n">pyspark</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">conf</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;es.resource&quot;</span> <span class="p">:</span> <span class="s">&quot;index/type&quot;</span><span class="p">}</span>   <span class="c"># assume Elasticsearch is running on localhost defaults</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">newAPIHadoopRDD</span><span class="p">(</span><span class="s">&quot;org.elasticsearch.hadoop.mr.EsInputFormat&quot;</span><span class="p">,</span>\
    <span class="s">&quot;org.apache.hadoop.io.NullWritable&quot;</span><span class="p">,</span> <span class="s">&quot;org.elasticsearch.hadoop.mr.LinkedMapWritable&quot;</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>         <span class="c"># the result is a MapWritable that is converted to a Python dict</span>
<span class="p">(</span><span class="s">u&#39;Elasticsearch ID&#39;</span><span class="p">,</span>
 <span class="p">{</span><span class="s">u&#39;field1&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
  <span class="s">u&#39;field2&#39;</span><span class="p">:</span> <span class="s">u&#39;Some Text&#39;</span><span class="p">,</span>
  <span class="s">u&#39;field3&#39;</span><span class="p">:</span> <span class="mi">12345</span><span class="p">})</span></code></pre></div>

    <p>需要注意的是，如果InputFormat仅仅是简单的依赖与一个Hadoop配置和/或一个输入路径，并且key和value类可以很容易的按照上面的表哥进行转化的话，
那么这个方法对这类情形来说可以很好的工作。</p>

    <p>如果你有自定义序列化的二进制数据(比如从Cassandra/HBase中加载的数据)，那么你需要首先将其转换为可以被Pyrolite pickler处理的Scala/Java类型。
<a href="api/scala/index.html#org.apache.spark.api.python.Converter">Converter</a>可以用来做这类事情。只需要扩展这个trait然后在<code>convert</code>
方法里实现你的转换代码即可。但要注意保证用以访问你的<code>InputFormat</code>的该类以及其需要的依赖要打入你的Spark job jar中，并被引入PySpark的类路径里。</p>

    <p>如果想查看使用Cassandra / HBase <code>InputFormat</code> 和 <code>OutputFormat</code>自定义转换器的例子，请看
<a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python examples</a>以及
<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters">Converter examples</a>。</p>

  </div>
</div>

<h2 id="rdd">RDD操作</h2>

<p>RDD支持两类操作：<em>transformations</em>，用以从一个已存的数据集中创建一个新的数据集，以及 <em>actions</em>，用以在一个数据集上执行了一系列计算后来往驱动程序中返回一个值。
例如，<code>map</code>操作就是一个transformation操作，该操作通过将每个数据集元素传递给一个函数然后返回一个新的RDD来容纳结果。另一方面，<code>reduce</code>则是一个<code>action</code>操作，该
操作将会通过一个函数来对RDD元素进行聚集操作，并将最终结果返回给驱动程序(尽管同样也存在一个并行的<code>reduceByKey</code>操作来返回一个分布式数据集)。</p>

<p>Spark中所有transformation都是<i>懒惰的</i>，意味着它们不会马上来计算结果。相反，它们仅仅是记录下来对基础数据集(例如一个文件)的transformation操作。
transformation操作仅当驱动程序的action操作需要返回一个结果的时候才会去进行计算。这种设计可以让Spark更高效的运行 &#8211; 例如，我们应该更倾向于
通过<code>map</code>操作创建的一个数据集被用于一个<code>reduce</code>操作，然后仅仅将<code>reduce</code>操作的结果返回给驱动程序，而不是去获得一个非常大的map数据集。</p>

<p>默认情况下，每个转换后(transformed)的RDD可以通过重复的在其上运行action来进行重复计算。但是你也可以通过<code>persist</code>(或<code>cache</code>)方法将RDD<code>持久化</code>到内存中，
在这种情况下，Spark将会在集群中保留这些元素以便允许你下次查询时非常快的获得到它们。同样的，将RDD持久化到磁盘上或在多个集群节点中进行复制也是支持的。</p>

<h3 id="section-3">基础</h3>

<div class="codetabs">

<div data-lang="scala">

    <p>为了展示RDD的基础概念，先让我们看一个简单的程序：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">lineLengths</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">)</span>
<span class="k">val</span> <span class="n">totalLength</span> <span class="k">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span></code></pre></div>

    <p>第一行使用外部文件定义了一个基本的RDD。这个数据集并没有真正的被载入内存或其他可用设备中：<code>lines</code>仅仅是一个该文件的指针而已。
第二行将一个<code>map</code>转换的结果定义为一个<code>lineLengths</code>的变量。同样的，<code>lineLengths</code>并<em>不是</em>被立即被计算的，而是要作为延迟计算的。
最后，我们运行一个<code>reduce</code>操作。在这时Spark将被实际计算工作分拆为运行在多个机器上的子任务，然后每台机器自行运算其分配的一部分map
计算和其本地的规约(reduction)计算，最后仅将其自己的计算结果返回给驱动程序。</p>

    <p>如果我们同样想在后续计算中使用<code>lineLengths</code>，我们可以在<code>reduce</code>前添加以下代码：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span class="o">()</span></code></pre></div>

    <p>这将导致<code>lineLengths</code>在其第一次被计算后被保存到内存中。</p>

  </div>

<div data-lang="java">

    <p>为了展示RDD的基础概念，先让我们看一个简单的程序：</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">());</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span></code></pre></div>

    <p>第一行使用外部文件定义了一个基本的RDD。这个数据集并没有真正的被载入内存或其他可用设备中：<code>lines</code>仅仅是一个该文件的指针而已。
第二行将一个<code>map</code>转换的结果定义为一个<code>lineLengths</code>的变量。同样的，<code>lineLengths</code>并<em>不是</em>被立即被计算的，而是要作为延迟计算的。
最后，我们运行一个<code>reduce</code>操作。在这时Spark将被实际计算工作分拆为运行在多个机器上的子任务，然后每台机器自行运算其分配的一部分map
计算和其本地的规约(reduction)计算，最后仅将其自己的计算结果返回给驱动程序。</p>

    <p>如果我们同样想在后续计算中使用<code>lineLengths</code>，我们可以在<code>reduce</code>前添加以下代码：</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">lineLengths</span><span class="o">.</span><span class="na">persist</span><span class="o">(</span><span class="n">StorageLevel</span><span class="o">.</span><span class="na">MEMORY_ONLY</span><span class="o">());</span></code></pre></div>

    <p>这将导致<code>lineLengths</code>在其第一次被计算后被保存到内存中。</p>

  </div>

<div data-lang="python">

    <p>为了展示RDD的基础概念，先让我们看一个简单的程序：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre></div>

    <p>第一行使用外部文件定义了一个基本的RDD。这个数据集并没有真正的被载入内存或其他可用设备中：<code>lines</code>仅仅是一个该文件的指针而已。
第二行将一个<code>map</code>转换的结果定义为一个<code>lineLengths</code>的变量。同样的，<code>lineLengths</code>并<em>不是</em>被立即被计算的，而是要作为延迟计算的。
最后，我们运行一个<code>reduce</code>操作。在这时Spark将被实际计算工作分拆为运行在多个机器上的子任务，然后每台机器自行运算其分配的一部分map
计算和其本地的归约(reduction)计算，最后仅将其自己的计算结果返回给驱动程序。</p>

    <p>如果我们同样想在后续计算中使用<code>lineLengths</code>，我们可以在<code>reduce</code>前添加以下代码：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span class="p">()</span></code></pre></div>

    <p>这将导致<code>lineLengths</code>在其第一次被计算后被保存到内存中。</p>

  </div>

</div>

<h3 id="spark-2">向Spark传递函数</h3>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark API重度依赖于向驱动程序中传递函数用以在集群中来运行。建议使用以下两种方式来这样做：</p>

    <ul>
      <li><a href="http://docs.scala-lang.org/tutorials/tour/anonymous-function-syntax.html">匿名函数(Anonymous function syntax</a>，可以用在小段代码中。</li>
      <li>在全局单例对象(global singleton object)中使用静态方法。比如，你可以定义<code>ojbect MyFunctions</code>以及对应函数，然后将函数传入，就像下面这样：</li>
    </ul>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">object</span> <span class="nc">MyFunctions</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">func1</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
<span class="o">}</span>

<span class="n">myRdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="nc">MyFunctions</span><span class="o">.</span><span class="n">func1</span><span class="o">)</span></code></pre></div>

    <p>注意同样有可能将一个类(区别于一个单例对象)中的函数引用传递给Spark，但这需要将该类的实际对象及其函数引用传入。例如以下这样：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">func1</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">func1</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span></code></pre></div>

    <p>这里，如果我们使用<code>new MyClass</code>创建一个对象然后调用它的<code>doStuff</code>函数，那么里面的<code>map</code>将会引用到<em>这个<code>MyClass</code>对象实例</em>的<code>func1</code>方法，这样
的话整个对象需要被发送到集群中。这跟写<code>rdd.map(x =&gt; this.func1(x))</code>是类似的。</p>

    <p>类似的，访问对象的成员变量也将会引用整个对象：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field</span> <span class="k">=</span> <span class="s">&quot;Hello&quot;</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">field</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span></code></pre></div>

    <p>这段代码与<code>rdd.map(x =&gt; this.field + x)</code>相同，它们都会引用整个<code>this</code>对象。如果想避免这种情况，最简单的方法就是拷贝<code>field</code>到一个本地变量来使用，
而不是直接使用它：</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field_</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="n">field</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">field_</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span>
<span class="o">}</span></code></pre></div>

  </div>

<div data-lang="java">

    <p>Spark API重度依赖于向驱动程序中传递函数用以在集群中来运行。在Java中，函数式接口需要通过实现
<a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>包下的接口来实现。
有下两种方式来创建函数</p>

    <ul>
      <li>让你的类实现Function接口，无论是匿名内部类还是命名的类，让后将其实例传递Spark。</li>
      <li>在Java 8下，使用<a href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html">lambda表达式</a>来简洁的定义一个函数实现。</li>
    </ul>

    <p>虽然这篇指南中使用了lambda语法来更简洁的表示，但使用相同的长格式(long-form)API方式来实现也很容易。比如，我们可以像以下这样的方式来书写代码：</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">();</span> <span class="o">}</span>
<span class="o">});</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span class="o">(</span><span class="k">new</span> <span class="n">Function2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">Integer</span> <span class="n">a</span><span class="o">,</span> <span class="n">Integer</span> <span class="n">b</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">;</span> <span class="o">}</span>
<span class="o">});</span></code></pre></div>

    <p>或者，如果使用内联函数比较笨拙的话，可以这样：</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">class</span> <span class="nc">GetLength</span> <span class="kd">implements</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">();</span> <span class="o">}</span>
<span class="o">}</span>
<span class="kd">class</span> <span class="nc">Sum</span> <span class="kd">implements</span> <span class="n">Function2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">Integer</span> <span class="n">a</span><span class="o">,</span> <span class="n">Integer</span> <span class="n">b</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">;</span> <span class="o">}</span>
<span class="o">}</span>

<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="nf">GetLength</span><span class="o">());</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span class="o">(</span><span class="k">new</span> <span class="nf">Sum</span><span class="o">());</span></code></pre></div>

    <p>需要注意的是Java的匿名内部类可以访问被标注为<code>final</code>的在其外部的变量。Spark将会把这些变量的拷贝传递给每个工作节点，就像其他语言那样。</p>

  </div>

<div data-lang="python">

    <p>Spark API重度依赖于向驱动程序中传递函数用以在集群中来运行。建议使用以下三种方式来这样做：</p>

    <ul>
      <li><a href="https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions">Lambda表达式</a>，一种可以用来声明简单函数的表达式。
(Lambda不支持多语句的函数或者不返回值的函数。)</li>
      <li>Local <code>def</code>s inside the function calling into Spark, for longer code.</li>
      <li>模块内的高级函数。</li>
    </ul>

    <p>例如，为了传递一个支持使用<code>lambda</code>的长函数，可以这样做：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="sd">&quot;&quot;&quot;MyScript.py&quot;&quot;&quot;</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">myFunc</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot; &quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;file.txt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">myFunc</span><span class="p">)</span></code></pre></div>

    <p>需要注意的是有可能将一个类(区别于一个单例对象)中的函数引用传递给Spark，但这需要将该类的实际对象及其函数引用传入。例如以下这样：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">s</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">)</span></code></pre></div>

    <p>这里，如果我们使用<code>new MyClass</code>创建一个对象然后调用它的<code>doStuff</code>函数，那么里面的<code>map</code>将会引用到<em>这个<code>MyClass</code>对象实例</em>的<code>func</code>方法，这样
的话整个对象需要被发送到集群中。</p>

    <p>类似的，访问对象的成员变量也将会引用整个对象：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">=</span> <span class="s">&quot;Hello&quot;</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span></code></pre></div>

    <p>如果想避免这种情况，最简单的方法就是拷贝<code>field</code>到一个本地变量来使用，
而不是直接使用它：</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="n">field</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">field</span>
    <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">field</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span></code></pre></div>

  </div>

</div>

<h3 id="a-nameclosureslinka">理解闭包 <a name="ClosuresLink"></a></h3>
<p>Spark的难点之一就是如何理解运行在集群中的代码中的变量和方法的作用域和生命周期。在变量作用域外改变RDD的操作在Spark中是一个经常导致理解困难的地方。
在下面的例子中，我们将会看到使用<code>foreach()</code>来增加计数器值的代码，但同样的问题也会发生在其他操作中。</p>

<h4 id="section-4">示例</h4>

<p>考虑以下简单的RDD元素累加操作，无论是否其执行在相同的JVM中,它的行为都会有很大的不同。
一个通用的例子则是在<code>local</code>模式下运行Spark(<code>--master=local[n]</code>)，而不是将其部署在集群中(e.g. 通过spark-submit部署至YARN):</p>

<div class="codetabs">

<div data-lang="scala">

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">var</span> <span class="n">counter</span> <span class="k">=</span> <span class="mi">0</span>
<span class="k">var</span> <span class="n">rdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="c1">// Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span><span class="o">)</span>

<span class="n">println</span><span class="o">(</span><span class="s">&quot;Counter value: &quot;</span> <span class="o">+</span> <span class="n">counter</span><span class="o">)</span></code></pre></div>

  </div>

<div data-lang="java">

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">);</span> 

<span class="c1">// Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span><span class="o">);</span>

<span class="n">println</span><span class="o">(</span><span class="s">&quot;Counter value: &quot;</span> <span class="o">+</span> <span class="n">counter</span><span class="o">);</span></code></pre></div>

  </div>

<div data-lang="python">

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c"># Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Counter value: &quot;</span> <span class="o">+</span> <span class="n">counter</span><span class="p">)</span></code></pre></div>

  </div>

</div>

<h4 id="vs-">本地模式 vs. 集群模式</h4>

<p>最主要的挑战在于以上代码的行为是无法预料的。在一个单一JVM中的本的模式下，以上代码会在RDD中累加所有值并将其存入<strong>counter</strong>变量。这是因为RDD和<strong>counter</strong>变量都共存于驱动节点上的同一内存空间内。</p>

<p>但是，在<code>cluster</code>模式下，所发生的一切将更为复杂，并且以上代码可能无法按预期来工作。为了执行作业，Spark将RDD的操作流程拆分为一个个的任务(task) - 每个任务会被一个执行器(executor)来执行。
在执行之前，Spark将对<strong>闭包</strong>进行计算。闭包就是那些必须对executor可见的用来在RDD上进行其计算的(本例中为<code>foreach()</code>)变量和方法。这类闭包会被序列化然后发送到每个executor中。
在<code>本地(local)</code>模式中，因为只有一个executor，所以闭包会被共享至所有情况下。但在其他模式下，每个在分离的工作节点上的executor都会拥有一份该闭包的拷贝，而不是共享该闭包。</p>

<p>因而在这种情形下，闭包内的变量仅仅是拷贝，所以当<strong>counter</strong>在<code>foreach</code>函数中被引用时，它不再是那个在驱动节点上的<strong>counter</strong>变量了。当然驱动节点的内存中仍然还有一个<strong>counter</strong>变量，
但其已经对其他executor不再可见了！那些executor只能够看到通过序列化的闭包里传递过来的变量的拷贝。因此，<strong>counter</strong>变量最终的值将仍然是零，因为所有在<strong>counter</strong>上的操作都在针对那些
序列化后的闭包中的值上进行。</p>

<p>为了保证在这些情形下这类行为的正确性，我们应当使用<a href="#AccumLink"><code>累加器(Accumulator)</code></a>。Spark的累加器专门用来保证在任务执行被分配到一个集群中的多个节点时能够安全的更新其中的变量。
在本指南的累加器一节会专门探讨它。</p>

<p>总的来说，闭包 - 就像循环或本地化定义的方法那样，不应当被用于改变某些全局的状态。Spark不会对那些闭包之外的对象引用的改变行为作出任何定义或保证。
一些这样干的代码或许能在本地模式下运行，但这只是侥幸的并且在分布式模式下基本不会按照预期来运行。如果需要对全局状态进行聚合运算，请使用累加器。</p>

<h4 id="rdd-1">打印RDD中的元素</h4>
<p>另一个比较通用的情形是想要使用<code>rdd.foreach(println)</code>或<code>rdd.map(println)</code>打印出RDD中的元素。在单机中，它可以生成所预期的输出并打印出所有RDD中的元素。
但是
Another common idiom is attempting to print out the elements of an RDD using <code>rdd.foreach(println)</code> or <code>rdd.map(println)</code>。但是在<code>集群</code>模式下，
executor对<code>stdout</code>所执行的输出已经被重定向到了executor自身的<code>stdout</code>上，而不是驱动程序上。因而驱动程序上的<code>stdout</code>将不会对RDD有任何显示！如果想打印在
驱动程序上的所有元素，你可以使用<code>collect()</code>方法首先将RDD放回驱动程序节点上：<code>rdd.collect().foreach(println)</code>。这很可能会导致驱动程序内存溢出，因为
<code>collect()</code>方法会把整个集群内的RDD抓取到一个节点上；如果你仅仅是想打印RDD中的一小部分元素，更安全的方法是用<code>take()</code>：<code>rdd.take(100).foreach(println)</code>。</p>

<h3 id="key-value">使用key-value键值对</h3>

<div class="codetabs">

<div data-lang="scala">

    <p>大部分的Spark操作对包含各种类型对象的RDD均可以使用，但有一小部分特殊的操作仅能够在包含key-value健值对的RDD上使用。最常见的就是分布式&#8221;shuffle&#8221;操作，
例如通过键来进行分组(grouping)或聚合(aggregating)。</p>

    <p>在Scala里，这些操作可以直接用在含有<a href="http://www.scala-lang.org/api/2.10.4/index.html#scala.Tuple2">Tuple2</a>
(语言内建的元组对象, 通过<code>(a, b)</code>可以简单的构建)对象的RDD上。<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">PairRDDFunctions</a>
类里的key-value键值对操作也同样可以自动的包装tuples RDD。</p>

    <p>比如，以下代码在一个key-value键值对上使用<code>reduceByKey</code>操作来计算在文件中每一行文字出现次数:</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span></code></pre></div>

    <p>我们同样也可以使用<code>counts.sortByKey()</code>，比如，将键值对按照字母排序，并在最后通过<code>counts.collect()</code>来将它们作为一组对象返回给主驱动程序。</p>

    <p><strong>注意：</strong>当在键值对中使用自定义对象作为键时，必须保证同时拥有自定义的<code>equals()</code>方法和相匹配的<code>hashCode()</code>方法。详细情况请参看<a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()">Object.hashCode()
Documentation</a>。</p>

  </div>

<div data-lang="java">

    <p>大部分的Spark操作对包含各种类型对象的RDD均可以使用，但有一小部分特殊的操作仅能够在包含key-value健值对的RDD上使用。最常见的就是分布式&#8221;shuffle&#8221;操作，
例如通过键来进行分组(grouping)或聚合(aggregating)。</p>

    <p>在Java里，key-value键值对一般使用Scala标准库中的<a href="http://www.scala-lang.org/api/2.10.4/index.html#scala.Tuple2">scala.Tuple2</a>
你可以简单的通过调用<code>new Tuple2(a, b)</code>来创建一个tuple，并且使用<code>tuple._1()</code>和<code>tuple._2()</code>来访问其成员。</p>

    <p>key-value键值对成员的RDD一般使用<a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">JavaPairRDD</a>类来表示。你可以通过特殊版本
的<code>map</code>操作来从JavaRDD构建JavaPairRDD，比如<code>mapToPair</code>和<code>flatMapToPair</code>。JavaPairRDD既有普通标准RDD函数，也包含特殊的key-value类函数。</p>

    <p>比如，以下代码在一个key-value键值对上使用<code>reduceByKey</code>操作来计算在文件中每一行文字出现次数:</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nc">JavaRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="nc">JavaPairRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">mapToPair</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="nc">Tuple2</span><span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">));</span>
<span class="nc">JavaPairRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span></code></pre></div>

    <p>我们同样也可以使用<code>counts.sortByKey()</code>，比如，将键值对按照字母排序，并在最后通过<code>counts.collect()</code>来将它们作为一组对象返回给主驱动程序。</p>

    <p><strong>注意：</strong>当在键值对中使用自定义对象作为键时，必须保证同时拥有自定义的<code>equals()</code>方法和相匹配的<code>hashCode()</code>方法。详细情况请参看<a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()">Object.hashCode()
Documentation</a>。</p>

  </div>

<div data-lang="python">

    <p>大部分的Spark操作对包含各种类型对象的RDD均可以使用，但有一小部分特殊的操作仅能够在包含key-value健值对的RDD上使用。最常见的就是分布式&#8221;shuffle&#8221;操作，
例如通过键来进行分组(grouping)或聚合(aggregating)。</p>

    <p>在Python里，这些操作可以直接用在Python内建的tuple(例如<code>(1, 2)</code>)RDD上。只需简单的创建tuple然后调用你想要的操作即可。</p>

    <p>比如，以下代码在一个key-value键值对上使用<code>reduceByKey</code>操作来计算在文件中每一行文字出现次数:</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre></div>

    <p>我们同样也可以使用<code>counts.sortByKey()</code>，比如，将键值对按照字母排序，并在最后通过<code>counts.collect()</code>来将它们作为一组对象返回给主驱动程序。</p>

  </div>

</div>

<h3 id="transformation">转换(Transformation)</h3>

<p>下表列出了一些Spark中常用的转换操作。详情请参看RDD API文档
(<a href="api/scala/index.html#org.apache.spark.rdd.RDD">Scala</a>,
 <a href="api/java/index.html?org/apache/spark/api/java/JavaRDD.html">Java</a>,
 <a href="api/python/pyspark.html#pyspark.RDD">Python</a>,
 <a href="api/R/index.html">R</a>)
以及键值对RDD函数文档
(<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">Scala</a>,
 <a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">Java</a>)</p>

<table class="table">
<tr><th style="width:25%">Transformation</th><th>Meaning</th></tr>
<tr>
  <td> <b>map</b>(<i>func</i>) </td>
  <td> 返回一个新建的分布式数据集，构建该集合需要通过一个函数<i>func</i>来将数据源的每一个元素导入。 </td>
</tr>
<tr>
  <td> <b>filter</b>(<i>func</i>) </td>
  <td> 返回一个新建的分布式数据集，构建该集合需要通过函数<i>func</i>来过滤数据源元素，并将符合返回值为true的元素进行导入。</td>
</tr>
<tr>
  <td> <b>flatMap</b>(<i>func</i>) </td>
  <td> 类似于map，但每一个输入项可以被map到0个或多个输出项上。(这样的话<i>func</i>可以返回一个Seq而不是一个单个的值)。 </td>
</tr>
<tr>
  <td> <b>mapPartitions</b>(<i>func</i>) <a name="MapPartLink"></a> </td>
  <td> 类似于map，但会在RDD的每个分区partition(或块block)上独立的运行，所以当在类型T的RDD上运行时，<i>func</i>必须是
    Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;类型   </td>
</tr>
<tr>
  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>
  <td> 类似于mapPartitions，但还提供了一个包含用来表示分区(partition)索引的整型值的<i>func</i>函数，所以当在类型T的RDD上运行时，<i>func</i>必须是
  (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;类型。
  </td>
</tr>
<tr>
  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>
  <td> 根据<i>fraction</i>指定的精度，对数据进行小规模取样，可选择是否用随机数替换，<i>seed</i>用于指定随机数生成器的种子。 </td>
</tr>
<tr>
  <td> <b>union</b>(<i>otherDataset</i>) </td>
  <td> 返回一个包含当前数据集和参数<i>otherDataset</i>中所有元素的并集的新数据集。 </td>
</tr>
<tr>
  <td> <b>intersection</b>(<i>otherDataset</i>) </td>
  <td> 返回一个包含当前数据集和参数<i>otherDataset</i>中所有元素的交集的新数据集。 </td>
</tr>
<tr>
  <td> <b>distinct</b>([<i>numTasks</i>])) </td>
  <td> 返回一个包含当前数据集中所有不重复元素的新数据集。</td>
</tr>
<tr>
  <td> <b>groupByKey</b>([<i>numTasks</i>]) <a name="GroupByLink"></a> </td>
  <td> 在一个（K, V）对的数据集上调用，返回一个(K，Iterable&lt;V&gt;)对的数据集。<br />
       <b>注意：</b>如果你使用grouping来对每个key执行聚集操作(比如sum或average)，使用<code>reduceByKey</code>或<code>aggregateByKey</code>
       会获得更好的性能。
       <b>注意：</b>默认情况下，并行任务数要依赖于父RDD的分区数目。你可以传入一个可选的numTasks参数来改变任务数。
  </td>
</tr>
<tr>
  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) <a name="ReduceByLink"></a> </td>
  <td> 在一个(K，V)对的数据集上调用时，返回一个(K，V)对的数据集，使用指定的reduce函数<i>func</i>，将相同key的值聚合到一起。<i>func</i>应当是(V,V) =&gt; V 类型的。
  类似<code>groupByKey</code>，reduce任务个数是可以通过第二个可选参数来配置的。</td>
</tr>
<tr>
  <td> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numTasks</i>]) <a name="AggregateByLink"></a> </td>
  <td> 在一个(K，V)对的数据集上调用时，返回一个(K,U)的数据集，使用指定的组合函数以及一个中立的"零"值。允许一个与输入值类型不同聚合值类型，用于避免没必要的分配。与<code>groupByKey</code>相似，reduce任务的数量可以通过可选的第二个参数来指定。</td>
</tr>
<tr>
  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>]) <a name="SortByLink"></a> </td>
  <td> 在一个(K, V)对(K是可排序的)的数据集上调用时，返回一个根据key升/降序排列的(K, V)数据集，通过<code>ascending</code>参数来指定升序还是降序。</td>
</tr>
<tr>
  <td> <b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name="JoinLink"></a> </td>
  <td> 在一个(K, V)数据集和一个(K, W)数据集上调用时，返回一个以K为key的所有V, W元素组成的tuple为值的(K, (V, W))型数据集。外连接可以通过
	<code>leftOuterJoin</code>, <code>rightOuterJoin</code>以及<code>fullOuterJoin</code>来实现。
  </td>
</tr>
<tr>
  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name="CogroupLink"></a> </td>
  <td> 在一个(K, V)数据集和一个(K, W)数据集上调用时，返回一个(K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;))类型的数据集。该操作也被叫做<code>groupWith</code>。 </td>
</tr>
<tr>
  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>
  <td> 在一个T类型数据集和一个U类型数据集上调用时，返回一个(T, U)类型的数据集(包含所有T和U元素)。(笛卡尔积)</td>
</tr>
<tr>
  <td> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>
  <td> 通过一个shell命令的管道来传递RDD的每个分区，可以是perl或bash命令。 RDD元素以字符串的行是被写入到进程的标准输入stdin然后按行输出至标准输出stdout中。 </td>
</tr>
<tr>
  <td> <b>coalesce</b>(<i>numPartitions</i>) <a name="CoalesceLink"></a> </td>
  <td> 将RDD的分区数目减少至numPartitions。适用于需要在过滤完一个大的数据集后更有效率的运行。</td>
</tr>
<tr>
  <td> <b>repartition</b>(<i>numPartitions</i>) </td>
  <td> 对RDD内数据随机的重新进行shuffle以创建更多或更少的分区，然后将数据重新平均分配至这些分区。该操作总会将整个网络内的数据进行shuffle。 <a name="RepartitionLink"></a></td>
</tr>
<tr>
  <td> <b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name="Repartition2Link"></a></td>
  <td> 针对指定的partitioner来重新分配RDD的分区，然后在每个重新分配的分区内将数据按key重新排序。该操作要比直接调用<code>repartition</code>后再在每个分区内进行排序操作要有效率的多，因为它将排序操作下调进了shuffle操作中。
  </td>
</tr>
</table>

<h3 id="actions">Actions</h3>

<p>下表列出了一些Spark里常用的Action。详情请参看RDD API文档
(<a href="api/scala/index.html#org.apache.spark.rdd.RDD">Scala</a>,
 <a href="api/java/index.html?org/apache/spark/api/java/JavaRDD.html">Java</a>,
 <a href="api/python/pyspark.html#pyspark.RDD">Python</a>,
 <a href="api/R/index.html">R</a>)</p>

<p>以及键值对RDD函数文档
(<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">Scala</a>,
 <a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">Java</a>)</p>

<table class="table">
<tr><th>Action</th><th>Meaning</th></tr>
<tr>
  <td> <b>reduce</b>(<i>func</i>) </td>
  <td> 通过函数func(接受两个参数，返回一个结果)对数据集中的所有元素进行聚合。这个功能必须可交换(commutative)且可关联(associative)的，从而可以正确的被并行执行。</td>
</tr>
<tr>
  <td> <b>collect</b>() </td>
  <td> 在驱动程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作并返回一个足够小的数据子集后再使用会比较有用。</td>
</tr>
<tr>
  <td> <b>count</b>() </td>
  <td> 	返回数据集的元素的个数。 </td>
</tr>
<tr>
  <td> <b>first</b>() </td>
  <td> 	返回数据集的第一个元素(类似于take(1))。 </td>
</tr>
<tr>
  <td> <b>take</b>(<i>n</i>) </td>
  <td> 	返回一个由数据集的前<i>n</i>个元素组成的数组。 </td>
</tr>
<tr>
  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>
  <td> 返回一个数组，在数据集中随机采样<i>num</i>个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定的随机数生成器种子。</td>
</tr>
<tr>
  <td> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>
  <td> 返回数据集前<i>n</i>个元素，可以选择是使用其自然的顺序或指定一个自定义comparator来排序。</td>
</tr>
<tr>
  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>
  <td> 将数据集的元素，以文本文件的形式，保存到本地文件系统目录，HDFS或者任何其它hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行。 </td>
</tr>
<tr>
  <td> <b>saveAsSequenceFile</b>(<i>path</i>) <br /> (Java and Scala) </td>
  <td> 将数据集的元素，以Hadoop sequencefile的格式，保存到本地系统指定的目录，HDFS或者任何其它hadoop支持的文件系统。这个只限于由key-value对组成，并实现了Hadoop的Writable接口的RDD。在Scala中，还可以是可以隐式转换为Writable的RDD。（Spark包括了基本类型的转换，例如Int，Double，String，等等）。 </td>
</tr>
<tr>
  <td> <b>saveAsObjectFile</b>(<i>path</i>) <br /> (Java and Scala) </td>
  <td> 使用Java序列化将数据集里的元素进行保存，之后可以使用<code>SparkContex.objectFile()</code>反序列化为原数据集。 </td>
</tr>
<tr>
  <td> <b>countByKey</b>() <a name="CountByLink"></a> </td>
  <td> 仅可以使用在(K, V)类型的RDD上。返回一个根据key来将V进行累加的(K, Int)hashmap。 </td>
</tr>
<tr>
  <td> <b>foreach</b>(<i>func</i>) </td>
  <td> 对数据集的每个元素运行函数<i>func</i>。一般会针对某些操作带来副作用，如更新一个<a href="#AccumLink">Accumulator</a>，或与外部存储系统的交互。<br />
  <b>注意：</b>更新除Accumulator以外并且在foreach()函数作用域外的变量可能导致一些无法预料的行为。详情请参看<a href="#ClosuresLink">理解闭包</a>。</td>
</tr>
</table>

<h3 id="shuffle">Shuffle操作</h3>

<p>Spark中某些特定操作会处罚一个事件，这个事件被称作shuffle。suffle是spark中的重新分配数据的一种机制，这样可以使数据在分区上被分为不同的组。这种机制比较典型的包括在执行器(executor)和机器之间拷贝数据，从而使得shuffle变为一个复杂且成本较高的操作。</p>

<h4 id="section-5">背景</h4>

<p>为了理解shuffle过程，让我们来以<a href="#ReduceByLink"><code>reduceByKey</code></a>操作为例来进行考虑。 <code>reduceByKey</code>操作会产生一个新的RDD，其会根据key来将vlue值进行聚合，然后生成新的tuple - 原key作为新key以及针对关联到这个key上的value值执行reduce函数后的结果作为新的value。其挑战在于某个key所关联的所有value值并不一定位于同一个分区上，甚至不在同一个机器上，但是它们必须要重定位到相同位置上以进行计算。</p>

<p>在Spark中，在进行一个固定操作时，数据一般会在固定的地方，而不是分布在不同的分区中。在计算过程中，单个任务会在单独的分区中执行 - 因此，为了组织所有数据来执行一个单独的&#8217;reduceByKey&#8217;的reduce任务，Spark需要去执行一个all-to-all操作。它必须从所有分区中读取并找到所有key值以及针对的所有value值，然后将所有分区上的针对每一个key的所有value值聚集在一起来计算出最终结果 - 这就被称为<strong>shuffle</strong>操作。</p>

<p>虽然在每个分区上新shuffle后的数据集元素是确定的，以及分区顺序也是确定的，可这些元素的顺序确不能确定的。如果需要在shuffle中可预测数据顺序，可能需要使用：</p>

<ul>
  <li><code>mapPartitions</code>来将每个使用的分区进行排序，例如，<code>.sorted</code></li>
  <li><code>repartitionAndSortWithinPartitions</code>来高效的在重新分区(repartitioning)的同时高效的对分区进行排序。</li>
  <li><code>sortBy</code>来产生一个全局排序的RDD。</li>
</ul>

<p>可能会导致shuffle的操作包括<strong>重分区(repartition)</strong>操作，比如<a href="#RepartitionLink"><code>repartition</code></a>和<a href="#CoalesceLink"><code>coalesce</code></a>；<strong>&#8216;ByKey</strong>操作(除counting操作外)，比如<a href="#GroupByLink"><code>groupByKey</code></a>和<a href="#ReduceByLink"><code>reduceByKey</code></a>；以及<strong>join</strong>操作，比如<a href="#CogroupLink"><code>cogroup</code></a>以及<a href="#JoinLink"><code>join</code></a>。</p>

<h4 id="section-6">性能影响</h4>
<p><strong>shuffle</strong>是一种昂贵的操作，因为它包含了磁盘I/O，数据序列化以及网络I/O。为了为shuffle组织数据，Spark会生成任务集合 - 一组<em>map</em>任务集合来组织数据，以及一组<em>reduce</em>任务来聚合数据。这个属于来自于MapReduce而且与Spark中的<code>map</code>和<code>reduce</code>操作并不是相关联的。</p>

<p>在内部，单独的map任务所产生的结果会被保存在内存中，直到内存无法保存为止。然后，它们会基于目标分区进行排序并被写入到一个单独的文件中。在reduce操作中，任务会读取相关的有序快。</p>

<p>特定的shuffle操作会消耗掉大量的堆内存，因为它们传输数据之前或之后，会使用了内存数据结构来组织数据。特别的，<code>reduceByKey</code>以及<code>aggregateByKey</code>会在map操作时构建这些数据结构，然后<code>'ByKey</code>操作会在reduce操作时真正产生这些结构。当数据不再适合放在内存时，Spark将会将这些表&#8221;溢出&#8221;到磁盘上，并伴随着更多额外的磁盘I/O以及更多的垃圾回收。</p>

<p>shuffle同样还会在磁盘上产生很多的中间文件。在Spark 1.3中，这些文件会被一直保存直到相应的RDD不再被使用并被垃圾回收。这样的话，如果需要重新计算，这些shuffle文件就没有必要被重新创建。如果应用程序一直保留了对这些RDD的引用，或者垃圾回收没有很频繁的清理的话，垃圾回收可能仅仅在很长一段时间后会发生。这意味着长时间运行的Spark任务会消耗很大一块内存空间。临时存储目录可以在配置Spark context时通过<code>spark.local.dir</code>配置参数来指定。</p>

<p>shuffle行为可以通过调整一些列配置参数来进行优化。请参看<a href="configuration.html">Spark Configuration Guide</a>内的&#8217;Shuffle行为&#8217;一节。</p>

<h2 id="rdd-2">RDD持久化</h2>

<p>Spark最重要的一个功能就是可以<em>持久化</em>(或<em>缓存</em>)数据集到内存中。当你持久化一个RDD时，每一个结点都将把它的计算分块结果保存在内存中，并在对此数据集（或者衍生出的数据集）进行的其它action中重用。这将允许之后的action更快(一般会块10倍以上)。对于迭代算法以及快速交互来说，缓存是一个非常关键的工具。</p>

<p>你可以使用<code>persist()</code>或<code>cache()</code>方法将一个RDD标记为将要持久化。那么在这个RDD第一次使用一个action计算后，它将会被保存在结点上的内存中。Spark缓存是可以容错的 &#8211; 如果RDD的任何一个分区发生了丢失，那么它会自动的被使用之前用来生成它的transformation来重新计算。</p>

<p>另外，每个持久化的RDD可以被使用不同的<em>存储级别(storage level)</em>来进行保存，从而使你来将数据集保存至磁盘，或使用序列化java对象(以节省空间)来保存至内存，或在节点之间复制，甚至可以使用<a href="http://tachyon-project.org/">Tachyon</a>在堆内存外(OFF_HEAP)保存。这些级别可以通过给<code>persist()</code>方法传递一个<code>StorageLevel</code>对象(<a href="api/scala/index.html#org.apache.spark.storage.StorageLevel">Scala</a>,
<a href="api/java/index.html?org/apache/spark/storage/StorageLevel.html">Java</a>,
<a href="api/python/pyspark.html#pyspark.StorageLevel">Python</a>)来实现。而<code>cache()</code>方法是一个缩略的方法，它直接使用了默认的级别，也就是<code>StorageLevel.MEMORY_ONLY</code>(使用非序列化对象保存在内存中)。所有的保存级别如下：</p>

<table class="table">
<tr><th style="width:23%">存储级别</th><th>意义</th></tr>
<tr>
  <td> MEMORY_ONLY </td>
  <td> 将RDD作为反序列化的的Java对象存储在JVM中。如果RDD不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算。这是是默认级别。 </td>
</tr>
<tr>
  <td> MEMORY_AND_DISK </td>
  <td> 将RDD作为反序列化的的对象存储在JVM中。如果RDD不能被内存装下，超出的分区将被保存在硬盘上，并且在需要时被读取。 </td>
</tr>
<tr>
  <td> MEMORY_ONLY_SER </td>
  <td> 将RDD作为<i>序列化的</i>对象进行存储(每一分区占用一个字节数组)。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用<a href="tuning.html">快速序列化器(fast serializer)</a>，但在读取时会比较占用CPU。
  </td>
</tr>
<tr>
  <td> MEMORY_AND_DISK_SER </td>
  <td> 与MEMORY_ONLY_SER相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算。</td>
</tr>
<tr>
  <td> DISK_ONLY </td>
  <td> 只将RDD分区存储在硬盘上。</td>
</tr>
<tr>
  <td> MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.  </td>
  <td> 与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上。 </td>
</tr>
<tr>
  <td> OFF_HEAP (experimental) </td>
  <td> 将RDD以序列化的格式存储在<a href="http://tachyon-project.org">Tachyon</a>中。与MEMORY_ONLY_SER相比，OFF_HEAP消减了垃圾回收的开支，并且允许执行器(executor)变的更小并可以共享内存池，从而使其在拥有大的对内存的环境或多个并行应用的环境中更具吸引力。此外，因为RDD存在于Tachyon中，那么执行器(executor)的崩溃就不会导致内存中缓存的丢失。在这种模式下，在Tacnyon中的内存是不可废弃的。因而Tachyon不会尝试重建一个它已从内存中废弃掉的块(block)。如果你打算使用Tachyon来做堆外存储(`OFF_HEAP`)，Spark与Tachyon的兼容性是开箱即用的。请参看<a href="http://tachyon-project.org/master/Running-Spark-on-Tachyon.html">该页面</a>来查看建议的版本匹配。
  </td>
</tr>
</table>

<p><strong>注意：</strong> <em>在python中，存储的对象总会使用<a href="https://docs.python.org/2/library/pickle.html">Pickle</a>库来进行序列化，因而无论你是否选择了一个序列化级别都没有关系。</em></p>

<p>Spark同样可以自动持久化一些shuffle操作的中间数据(比如<code>reduceByKey</code>），即使用户没有调用<code>persist</code>。这样做是为了在shuffle过程中节点出问题的情况下避免重新计算整个输入集。但我们还是建议如果打算重用结果RDD的话，用户要直接对其调用<code>persist</code>。</p>

<h3 id="section-7">存储级别的选择</h3>

<p>Spark的不同存储级别，旨在满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p>

<ul>
  <li>
    <p>如果你的RDD可以很好的与默认的存储级别(<code>MEMORY_ONLY</code>)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDD的操作尽可能的快。</p>
  </li>
  <li>
    <p>如果不行，试着使用<code>MEMORY_ONLY_SER</code>并且选择一个<a href="tuning.html">快速序列化的库</a>使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p>
  </li>
  <li>
    <p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p>
  </li>
  <li>
    <p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。<em>所有</em>的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p>
  </li>
  <li>
    <p>在大内存或者多应用的环境中， 实验性的<code>OFF_HEAP</code>模式有几个好处：</p>
    <ul>
      <li>允许多个执行器(executor)共享Tachyon中的共享内存。</li>
      <li>显著的减少垃圾回收的消耗。</li>
      <li>单个executor崩溃时缓存数据不会丢失。</li>
    </ul>
  </li>
</ul>

<h3 id="section-8">数据移除</h3>

<p>Spark 自动监控每个节点的缓存使用，依据最近最少使用算法(LRU)丢弃老的数据分区. 如果你想手工移除而不是等待cache移除机制，使<code>RDD.unpersist()</code>方法。</p>

<h1 id="section-9">共享变量</h1>

<p>一般来说，当一个函数被传递给Spark操作(例如map或reduce)，在一个远程集群节点上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器，在远程机器上对变量的所有更新都不会被传播回驱动程序。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式提供了两种有限的<em>共享变量</em>：广播变量(broadcast variable)和累加器(accumulator)。</p>

<h2 id="broadcast-variable">广播变量(broadcast variable)</h2>

<p>广播变量允许程序员保留一个只读的变量，缓存在每一台机器上，而非每个任务保存一份拷贝。它们可以这样被使用，例如，以一种高效的方式给每个结点一个大的输入数据集。Spark会尝试使用一种高效的广播算法来传播广播变量，从而减少通信的代价。</p>

<p>Spark action在一组被分布式&#8221;shuffle&#8221;操作分离开的场景中被执行。Spark会自动的将一些每个场景中任务所需要的通用数据进行广播。通过这种方式广播的数据会被以序列化的方式缓存，之后在运行每个任务前被反序列化。这意味着显式创建广播变量将仅对需要相同数据的多个场景中的任务有用，或只对那些以非序列化格式缓存数据的方式很重要的场景有用。</p>

<p>广播变量是通过调用<code>SparkContext.broadcast(v)</code>方法从变量<code>v</code>创建的。广播变量是一个<code>v</code>的封装器，它的值可以通过调用<code>value</code>方法获得。以下代码进行了展示：</p>

<div class="codetabs">

<div data-lang="scala">

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">broadcastVar</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span>
<span class="n">broadcastVar</span><span class="k">:</span> <span class="kt">org.apache.spark.broadcast.Broadcast</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">Broadcast</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">broadcastVar</span><span class="o">.</span><span class="n">value</span>
<span class="n">res0</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span></code></pre></div>

  </div>

<div data-lang="java">

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">Broadcast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">[]&gt;</span> <span class="n">broadcastVar</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">broadcast</span><span class="o">(</span><span class="k">new</span> <span class="kt">int</span><span class="o">[]</span> <span class="o">{</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">});</span>

<span class="n">broadcastVar</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
<span class="c1">// returns [1, 2, 3]</span></code></pre></div>

  </div>

<div data-lang="python">

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">broadcastVar</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&lt;</span><span class="n">pyspark</span><span class="o">.</span><span class="n">broadcast</span><span class="o">.</span><span class="n">Broadcast</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x102789f10</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">broadcastVar</span><span class="o">.</span><span class="n">value</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span></code></pre></div>

  </div>

</div>

<p>在广播变量被创建后，它应该在集群运行的任何函数中，代替<code>v</code>值被调用，从而v值不需要被再次传递到这些结点上。另外，对象<code>v</code>不能在广播后修改，这样可以保证所有结点的收到的都是一模一样的广播值(例如如果变量之后被传播到了一个新的节点)。</p>

<h2 id="accumulator-a-nameaccumlinka">累加器(Accumulator) <a name="AccumLink"></a></h2>

<p>累加器是一种只能通过关联操作进行“加”操作的变量，因此可以高效被并行支持。它们可以用来实现计数器counter(如MapReduce中)和求和器sum。Spark原生就支持数字numeric类型的累加器，开发者可以自己添加新的支持类型。如果创建累加器时给其进行了命名，它们将会被显示在Spark UI上。这对于理解运行场景的进度是很有用的(注意：暂时在Python中还不支持这项功能)。</p>

<p>一个累加器可以通过调用<code>SparkContext.accumulator(v)</code>方法从一个初始值<code>v</code>中创建。运行在集群上的任务，可以通过使用<code>add</code>方法或<code>+=</code>操作符(Scala和Python中)来给它加值。然而，他们不能读取这个值。只有驱动程序可以使用<code>value</code>方法来读取累加器的值。</p>

<p>以下代码展示了如何利用累加器来将一个数组里面的所有元素相加：</p>

<div class="codetabs">

<div data-lang="scala">

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">accum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="s">&quot;My Accumulator&quot;</span><span class="o">)</span>
<span class="n">accum</span><span class="k">:</span> <span class="kt">spark.Accumulator</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="mi">0</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="n">foreach</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span> <span class="o">+=</span> <span class="n">x</span><span class="o">)</span>
<span class="o">...</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">29</span> <span class="mi">18</span><span class="k">:</span><span class="err">41</span><span class="kt">:</span><span class="err">08</span> <span class="kt">INFO</span> <span class="kt">SparkContext:</span> <span class="kt">Tasks</span> <span class="kt">finished</span> <span class="kt">in</span> <span class="err">0</span><span class="kt">.</span><span class="err">317106</span> <span class="kt">s</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">value</span>
<span class="n">res2</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span></code></pre></div>

    <p>代码使用了内建的Int的累加器, 开发者也可以自己创建<a href="api/scala/index.html#org.apache.spark.AccumulatorParam">AccumulatorParam</a>的子类来建立新的累加器类型。AccumulatorParam接口有两个方法，<code>zero</code>代表为你的数据类型提供一个&#8221;零值&#8221;，addInPlace代表将两个值相加。 例如假定我们有一个<code>Vector</code>类，代表数学里的vector, 我们可以实现代码如下:</p>

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">object</span> <span class="nc">VectorAccumulatorParam</span> <span class="k">extends</span> <span class="nc">AccumulatorParam</span><span class="o">[</span><span class="kt">Vector</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">zero</span><span class="o">(</span><span class="n">initialValue</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Vector</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Vector</span><span class="o">.</span><span class="n">zeros</span><span class="o">(</span><span class="n">initialValue</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="k">def</span> <span class="n">addInPlace</span><span class="o">(</span><span class="n">v1</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">,</span> <span class="n">v2</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Vector</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">v1</span> <span class="o">+=</span> <span class="n">v2</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// 然后，创建一个该类型的累加器：</span>
<span class="k">val</span> <span class="n">vecAccum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="o">(</span><span class="k">new</span> <span class="nc">Vector</span><span class="o">(...))(</span><span class="nc">VectorAccumulatorParam</span><span class="o">)</span></code></pre></div>

    <p>如果使用Scala, Spark也支持更通用的<a href="api/scala/index.html#org.apache.spark.Accumulable">Accumulable</a>接口，可以用于累加数据，并且结果不一定与原始元素类型相同(比如通过收集所有元素来构建一个列表list)，并且Spark还支持使用<code>SparkContext.accumulableCollection</code>方法来累加通用的Scala集合类型。</p>

  </div>

<div data-lang="java">

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">Accumulator</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">accumulator</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>

<span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="na">foreach</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">x</span><span class="o">));</span>
<span class="c1">// ...</span>
<span class="c1">// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span>

<span class="n">accum</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
<span class="c1">// returns 10</span></code></pre></div>

    <p>代码使用了内建的Integer的累加器, 开发者也可以自己创建<a href="api/java/index.html?org/apache/spark/AccumulatorParam.html">AccumulatorParam</a>的子类来建立新的累加器类型。AccumulatorParam接口有两个方法，<code>zero</code>代表为你的数据类型提供一个&#8221;零值&#8221;，addInPlace代表将两个值相加。 例如假定我们有一个<code>Vector</code>类，代表数学里的vector, 我们可以实现代码如下:</p>

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">class</span> <span class="nc">VectorAccumulatorParam</span> <span class="kd">implements</span> <span class="n">AccumulatorParam</span><span class="o">&lt;</span><span class="n">Vector</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Vector</span> <span class="nf">zero</span><span class="o">(</span><span class="n">Vector</span> <span class="n">initialValue</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">Vector</span><span class="o">.</span><span class="na">zeros</span><span class="o">(</span><span class="n">initialValue</span><span class="o">.</span><span class="na">size</span><span class="o">());</span>
  <span class="o">}</span>
  <span class="kd">public</span> <span class="n">Vector</span> <span class="nf">addInPlace</span><span class="o">(</span><span class="n">Vector</span> <span class="n">v1</span><span class="o">,</span> <span class="n">Vector</span> <span class="n">v2</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">v1</span><span class="o">.</span><span class="na">addInPlace</span><span class="o">(</span><span class="n">v2</span><span class="o">);</span> <span class="k">return</span> <span class="n">v1</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// 然后，创建一个该类型的累加器：</span>
<span class="n">Accumulator</span><span class="o">&lt;</span><span class="n">Vector</span><span class="o">&gt;</span> <span class="n">vecAccum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">accumulator</span><span class="o">(</span><span class="k">new</span> <span class="nf">Vector</span><span class="o">(...),</span> <span class="k">new</span> <span class="nf">VectorAccumulatorParam</span><span class="o">());</span></code></pre></div>

    <p>如果使用Java, Spark也支持更通用的<a href="api/java/index.html?org/apache/spark/Accumulable.html">Accumulable</a>接口，可以用于累加数据，并且结果不一定与原始元素类型相同(比如通过收集所有元素来构建一个列表list)。</p>

  </div>

<div data-lang="python">

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Accumulator</span><span class="o">&lt;</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="o">...</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">29</span> <span class="mi">18</span><span class="p">:</span><span class="mi">41</span><span class="p">:</span><span class="mi">08</span> <span class="n">INFO</span> <span class="n">SparkContext</span><span class="p">:</span> <span class="n">Tasks</span> <span class="n">finished</span> <span class="ow">in</span> <span class="mf">0.317106</span> <span class="n">s</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">value</span>
<span class="mi">10</span></code></pre></div>

    <p>代码使用了内建的Int的累加器, 开发者也可以自己创建<a href="api/python/pyspark.html#pyspark.AccumulatorParam">AccumulatorParam</a>的子类来建立新的累加器类型。AccumulatorParam接口有两个方法，<code>zero</code>代表为你的数据类型提供一个&#8221;零值&#8221;，addInPlace代表将两个值相加。 例如假定我们有一个<code>Vector</code>类，代表数学里的vector, 我们可以实现代码如下:</p>

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">VectorAccumulatorParam</span><span class="p">(</span><span class="n">AccumulatorParam</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">zero</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initialValue</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Vector</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">initialValue</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addInPlace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
        <span class="n">v1</span> <span class="o">+=</span> <span class="n">v2</span>
        <span class="k">return</span> <span class="n">v1</span>

<span class="c"># 然后，创建一个该类型的累加器：</span>
<span class="n">vecAccum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="n">Vector</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">VectorAccumulatorParam</span><span class="p">())</span></code></pre></div>

  </div>

</div>

<p>For accumulator updates performed inside <b>actions only</b>, Spark guarantees that each task&#8217;s update to the accumulator 
will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware 
of that each task&#8217;s update may be applied more than once if tasks or job stages are re-executed.</p>

<p>Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like <code>map()</code>. The below code fragment demonstrates this property:</p>

<div class="codetabs">

<div data-lang="scala">

    <div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">accum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span> <span class="o">+=</span> <span class="n">x</span><span class="o">;</span> <span class="n">f</span><span class="o">(</span><span class="n">x</span><span class="o">)</span> <span class="o">}</span>
<span class="c1">// Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre></div>

  </div>

<div data-lang="java">

    <div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">Accumulator</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">accumulator</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
<span class="n">data</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="o">{</span> <span class="n">accum</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">x</span><span class="o">);</span> <span class="k">return</span> <span class="nf">f</span><span class="o">(</span><span class="n">x</span><span class="o">);</span> <span class="o">});</span>
<span class="c1">// Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre></div>

  </div>

<div data-lang="python">

    <div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="c"># Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre></div>

  </div>

</div>

<h1 id="section-10">发布到集群中</h1>

<p><a href="submitting-applications.html">应用提交指南</a>描述了如何提交应用到一个Spark集群中。简而言之，一旦你打包好你的应用jar包（Java/Scala)，或者一堆<code>.py</code>或<code>.zip</code>文件(Python), <code>bin/spark-submit</code>脚本允许你提交它们到任意的cluster manager。</p>

<h1 id="javascalasparkjob">从Java/Scala中启动Spark作业(job)</h1>

<p><a href="api/java/index.html?org/apache/spark/launcher/package-summary.html">org.apache.spark.launcher</a>包提供了用于将Spark作业启动为子进程的类，使用简单的Java API。
package provides classes for launching Spark jobs as child processes using a simple Java API.</p>

<h1 id="section-11">单元测试</h1>

<p>Spark针对任何流行的单元测试框架下的单元测试是友好的。只需在你的测试中创建一个<code>SparkContext</code>，并将master URL设置为<code>local</code>，然后就可以运行你的操作，并在最好调用<code>SparkContext.stop()</code>来结束测试即可。务必保证在一个<code>finally</code>块中或在测试框架的<code>tearDown</code>方法中结束context，因为Spark不支持在一个程序中同时并行的运行两个context。</p>

<h1 id="section-12">从1.0以前的版本升级</h1>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark 1.0 为1.x系列版本冻结了API的改动， 没有被标记为&#8221;experimental&#8221;或者&#8221;developer API&#8221;的API将会在未来版本中被支持。 对于Scala用户唯一的改变是分组操作，如<code>groupByKey</code>，<code>cogroup</code>和<code>join</code>，返回结果类型从<code>(Key, Seq[Value])</code>改为<code>(Key, Iterable[Value])</code>。</p>

  </div>

<div data-lang="java">

    <p>Spark 1.0 为1.x系列版本冻结了API的改动， 没有被标记为&#8221;experimental&#8221;或者&#8221;developer API&#8221;的API将会在未来版本中被支持。对Java API来说有以下改动：</p>

    <ul>
      <li><code>org.apache.spark.api.java.function</code>内的Function类在1.0中变为了interface，这意味着之前使用<code>extend Function</code>的旧代码现在需要使用<code>implement Function</code>来代替。</li>
      <li><code>map</code> transformation有新变化，例如<code>mapToPair</code>和<code>mapToDouble</code>，被加入近来用以创建新的特殊数据类型的RDD。</li>
      <li>分组操作例如<code>groupByKey</code>，<code>cogroup</code>以及<code>join</code>的返回值从<code>(Key, List&lt;Value&gt;)</code>变为<code>(Key, Iterable&lt;Value&gt;)</code>。</li>
    </ul>

  </div>

<div data-lang="python">

    <p>Spark 1.0 为1.x系列版本冻结了API的改动， 没有被标记为&#8221;experimental&#8221;或者&#8221;developer API&#8221;的API将会在未来版本中被支持。对Python用户来说，唯一的变动就是分组操作，例如<code>groupByKey</code>，<code>cogroup</code>以及<code>join</code>的返回值从(key, list of values)变为(key, iterable of values)。</p>

  </div>

</div>

<p>以下升级指南也同样可见<a href="streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x">Spark Streaming</a>，
<a href="mllib-guide.html#migration-guide">MLlib</a>和<a href="graphx-programming-guide.html#migrating-from-spark-091">GraphX</a>。</p>

<h1 id="section-13">更多资料</h1>

<p>你可以在官方站点查看<a href="http://spark.apache.org/examples.html">Spark示例程序</a>. 除此之外，Spark在发布包的<code>examples</code>的文件夹中包含了几个例子
(<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples">Scala</a>,
 <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples">Java</a>,
 <a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python</a>,
 <a href="https://github.com/apache/spark/tree/master/examples/src/main/r">R</a>)。
运行Java 和 Scala例子时你可以传递类名给Spark的<code>bin/run-example</code>脚本，例如:</p>

<pre><code>./bin/run-example SparkPi
</code></pre>

<p>对于Python例子，使用<code>spark-submit</code>：</p>

<pre><code>./bin/spark-submit examples/src/main/python/pi.py
</code></pre>

<p>对于R例子，使用<code>spark-submit</code>：</p>

<pre><code>./bin/spark-submit examples/src/main/r/dataframe.R
</code></pre>

<p>想了解优化的方法, <a href="configuration.html">配置(configuration)</a>和<a href="tuning.html">调优(tuning)</a>指南提供了最佳实践。特别重要的是你的数据要以有效的格式存储在内存中. 想了解发布信息，<a href="cluster-overview.html">cluster mode overview</a>描述了分布式操作和集群管理器支持的组件。</p>

<p>最后全部的API文档请访问<a href="api/scala/#org.apache.spark.package">Scala</a>, <a href="api/java/">Java</a>, <a href="api/python/">Python</a> and <a href="api/R/">R</a>。</p>


        </div> <!-- /container -->

        <script src="js/vendor/jquery-1.8.0.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
